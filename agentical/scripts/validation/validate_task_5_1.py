"""
Task 5.1 Validation Script - Bayesian Inference Integration

This script validates the implementation of Task 5.1: Bayesian Inference Integration
by checking the structure, completeness, and basic functionality of all components.

Validation includes:
- File existence and structure validation
- Module import verification (with dependency fallbacks)
- Component interface validation
- Configuration validation
- Documentation completeness check
"""

import os
import sys
import inspect
from pathlib import Path
from typing import Dict, List, Any, Optional


class Task5_1Validator:
    """Comprehensive validator for Task 5.1 implementation."""

    def __init__(self):
        self.base_path = Path(__file__).parent
        self.reasoning_path = self.base_path / "reasoning"
        self.validation_results = {
            "file_structure": {},
            "module_interfaces": {},
            "configuration_validation": {},
            "documentation_check": {},
            "overall_status": "pending"
        }

    def validate_all(self) -> Dict[str, Any]:
        """Run complete validation suite."""
        print("ğŸš€ Starting Task 5.1 Validation: Bayesian Inference Integration")
        print("=" * 80)

        # 1. Validate file structure
        print("\nğŸ“ Validating File Structure...")
        self.validate_file_structure()

        # 2. Validate module interfaces
        print("\nğŸ” Validating Module Interfaces...")
        self.validate_module_interfaces()

        # 3. Validate configurations
        print("\nâš™ï¸ Validating Configurations...")
        self.validate_configurations()

        # 4. Validate documentation
        print("\nğŸ“š Validating Documentation...")
        self.validate_documentation()

        # 5. Generate final report
        print("\nğŸ“Š Generating Final Report...")
        self.generate_final_report()

        return self.validation_results

    def validate_file_structure(self):
        """Validate that all required files exist with expected structure."""
        required_files = {
            "__init__.py": "Module initialization file",
            "bayesian_engine.py": "Core Bayesian inference engine",
            "belief_updater.py": "Belief updating component",
            "decision_tree.py": "Decision tree framework",
            "uncertainty_quantifier.py": "Uncertainty quantification module",
            "mcp_integration.py": "MCP server integration",
            "probabilistic_models.py": "Probabilistic models library"
        }

        test_files = {
            "test_task_5_1_bayesian_inference.py": "Comprehensive test suite"
        }

        results = {}

        # Check reasoning module directory
        if not self.reasoning_path.exists():
            results["reasoning_directory"] = {"status": "âŒ MISSING", "path": str(self.reasoning_path)}
            self.validation_results["file_structure"] = results
            return

        results["reasoning_directory"] = {"status": "âœ… EXISTS", "path": str(self.reasoning_path)}

        # Check required files in reasoning module
        for filename, description in required_files.items():
            file_path = self.reasoning_path / filename
            if file_path.exists():
                size = file_path.stat().st_size
                results[filename] = {
                    "status": "âœ… EXISTS",
                    "size_bytes": size,
                    "description": description
                }
                print(f"  âœ… {filename} ({size:,} bytes) - {description}")
            else:
                results[filename] = {
                    "status": "âŒ MISSING",
                    "description": description
                }
                print(f"  âŒ {filename} - MISSING")

        # Check test files
        for filename, description in test_files.items():
            file_path = self.base_path / filename
            if file_path.exists():
                size = file_path.stat().st_size
                results[filename] = {
                    "status": "âœ… EXISTS",
                    "size_bytes": size,
                    "description": description
                }
                print(f"  âœ… {filename} ({size:,} bytes) - {description}")
            else:
                results[filename] = {
                    "status": "âŒ MISSING",
                    "description": description
                }
                print(f"  âŒ {filename} - MISSING")

        self.validation_results["file_structure"] = results

    def validate_module_interfaces(self):
        """Validate module interfaces and class structures."""
        results = {}

        # Add reasoning path to sys.path for imports
        sys.path.insert(0, str(self.base_path))

        # Expected components and their key classes/functions
        expected_components = {
            "bayesian_engine": [
                "BayesianInferenceEngine",
                "BayesianConfig",
                "InferenceResult",
                "Evidence",
                "Hypothesis"
            ],
            "belief_updater": [
                "BeliefUpdater",
                "BeliefUpdaterConfig",
                "BeliefUpdate",
                "EvidenceType",
                "UpdateStrategy"
            ],
            "decision_tree": [
                "DecisionTree",
                "DecisionTreeConfig",
                "DecisionNode",
                "NodeType",
                "DecisionCriteria"
            ],
            "uncertainty_quantifier": [
                "UncertaintyQuantifier",
                "UncertaintyQuantifierConfig",
                "UncertaintyMeasure",
                "DistributionType",
                "QuantificationMethod"
            ],
            "mcp_integration": [
                "BayesMCPClient",
                "MCPConfig",
                "InferenceRequest",
                "InferenceResponse",
                "ServerStatus"
            ],
            "probabilistic_models": [
                "ProbabilisticModel",
                "BayesianNetwork",
                "MarkovChain",
                "HiddenMarkovModel",
                "GaussianProcess"
            ]
        }

        for module_name, expected_classes in expected_components.items():
            module_path = self.reasoning_path / f"{module_name}.py"

            if not module_path.exists():
                results[module_name] = {
                    "status": "âŒ FILE_MISSING",
                    "expected_classes": expected_classes,
                    "found_classes": []
                }
                continue

            # Read and analyze file content
            try:
                with open(module_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                found_classes = []
                missing_classes = []

                for class_name in expected_classes:
                    if f"class {class_name}" in content or f"{class_name} = " in content:
                        found_classes.append(class_name)
                    else:
                        missing_classes.append(class_name)

                if not missing_classes:
                    status = "âœ… COMPLETE"
                    print(f"  âœ… {module_name}: All {len(expected_classes)} components found")
                else:
                    status = "âš ï¸ PARTIAL"
                    print(f"  âš ï¸ {module_name}: {len(found_classes)}/{len(expected_classes)} components found")
                    print(f"    Missing: {', '.join(missing_classes)}")

                results[module_name] = {
                    "status": status,
                    "expected_classes": expected_classes,
                    "found_classes": found_classes,
                    "missing_classes": missing_classes,
                    "file_size": len(content),
                    "line_count": content.count('\n') + 1
                }

            except Exception as e:
                results[module_name] = {
                    "status": "âŒ READ_ERROR",
                    "error": str(e),
                    "expected_classes": expected_classes,
                    "found_classes": []
                }
                print(f"  âŒ {module_name}: Error reading file - {e}")

        self.validation_results["module_interfaces"] = results

    def validate_configurations(self):
        """Validate configuration classes and their parameters."""
        results = {}

        config_patterns = [
            ("BayesianConfig", ["confidence_threshold", "uncertainty_tolerance", "max_iterations"]),
            ("BeliefUpdaterConfig", ["default_strategy", "convergence_threshold", "stability_window"]),
            ("DecisionTreeConfig", ["max_depth", "max_branches_per_node", "confidence_threshold"]),
            ("UncertaintyQuantifierConfig", ["default_method", "confidence_levels", "monte_carlo_samples"]),
            ("MCPConfig", ["server_url", "timeout_seconds", "max_retries"]),
            ("ProbabilisticModelConfig", ["model_type", "inference_method", "max_training_iterations"])
        ]

        for config_name, expected_fields in config_patterns:
            found_config = False
            found_fields = []

            # Check all module files for this config
            for module_file in self.reasoning_path.glob("*.py"):
                try:
                    with open(module_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    if f"class {config_name}" in content:
                        found_config = True

                        # Check for expected fields
                        for field in expected_fields:
                            if field in content:
                                found_fields.append(field)

                        break
                except Exception:
                    continue

            if found_config:
                if len(found_fields) == len(expected_fields):
                    status = "âœ… COMPLETE"
                    print(f"  âœ… {config_name}: All {len(expected_fields)} fields found")
                else:
                    status = "âš ï¸ PARTIAL"
                    missing = set(expected_fields) - set(found_fields)
                    print(f"  âš ï¸ {config_name}: {len(found_fields)}/{len(expected_fields)} fields found")
                    print(f"    Missing: {', '.join(missing)}")
            else:
                status = "âŒ MISSING"
                print(f"  âŒ {config_name}: Configuration class not found")

            results[config_name] = {
                "status": status,
                "found": found_config,
                "expected_fields": expected_fields,
                "found_fields": found_fields
            }

        self.validation_results["configuration_validation"] = results

    def validate_documentation(self):
        """Validate documentation completeness."""
        results = {}

        documentation_checks = {
            "module_docstrings": 0,
            "class_docstrings": 0,
            "method_docstrings": 0,
            "type_hints": 0,
            "comprehensive_comments": 0
        }

        total_modules = 0
        total_classes = 0
        total_methods = 0

        for module_file in self.reasoning_path.glob("*.py"):
            if module_file.name.startswith("__"):
                continue

            try:
                with open(module_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                total_modules += 1

                # Check module docstring
                if content.strip().startswith('"""') or content.strip().startswith("'''"):
                    documentation_checks["module_docstrings"] += 1

                # Count classes and check docstrings
                class_count = content.count("class ")
                total_classes += class_count

                # Simple heuristic for class docstrings
                class_docstring_count = content.count('class ') if '"""' in content else 0
                documentation_checks["class_docstrings"] += min(class_docstring_count, class_count)

                # Count methods and check docstrings
                method_count = content.count("def ")
                total_methods += method_count

                # Simple heuristic for method docstrings
                method_docstring_count = content.count('def ') if '"""' in content else 0
                documentation_checks["method_docstrings"] += min(method_docstring_count, method_count)

                # Check for type hints
                if "from typing import" in content and "->" in content:
                    documentation_checks["type_hints"] += 1

                # Check for comprehensive comments
                comment_lines = [line for line in content.split('\n') if line.strip().startswith('#')]
                if len(comment_lines) > 10:  # Arbitrary threshold
                    documentation_checks["comprehensive_comments"] += 1

            except Exception as e:
                print(f"  âŒ Error checking documentation in {module_file.name}: {e}")

        # Calculate documentation scores
        doc_score = 0
        max_score = 0

        if total_modules > 0:
            module_doc_score = documentation_checks["module_docstrings"] / total_modules
            print(f"  ğŸ“– Module docstrings: {documentation_checks['module_docstrings']}/{total_modules} ({module_doc_score:.1%})")
            doc_score += module_doc_score
            max_score += 1

        if total_classes > 0:
            class_doc_score = min(documentation_checks["class_docstrings"] / total_classes, 1.0)
            print(f"  ğŸ“– Class docstrings: {documentation_checks['class_docstrings']}/{total_classes} ({class_doc_score:.1%})")
            doc_score += class_doc_score
            max_score += 1

        if total_methods > 0:
            method_doc_score = min(documentation_checks["method_docstrings"] / total_methods, 1.0)
            print(f"  ğŸ“– Method docstrings: {documentation_checks['method_docstrings']}/{total_methods} ({method_doc_score:.1%})")
            doc_score += method_doc_score
            max_score += 1

        type_hint_score = documentation_checks["type_hints"] / max(total_modules, 1)
        print(f"  ğŸ“– Type hints: {documentation_checks['type_hints']}/{total_modules} modules ({type_hint_score:.1%})")
        doc_score += type_hint_score
        max_score += 1

        comment_score = documentation_checks["comprehensive_comments"] / max(total_modules, 1)
        print(f"  ğŸ“– Comprehensive comments: {documentation_checks['comprehensive_comments']}/{total_modules} modules ({comment_score:.1%})")
        doc_score += comment_score
        max_score += 1

        overall_doc_score = doc_score / max_score if max_score > 0 else 0

        if overall_doc_score >= 0.8:
            doc_status = "âœ… EXCELLENT"
        elif overall_doc_score >= 0.6:
            doc_status = "âœ… GOOD"
        elif overall_doc_score >= 0.4:
            doc_status = "âš ï¸ ADEQUATE"
        else:
            doc_status = "âŒ POOR"

        print(f"  ğŸ“Š Overall documentation score: {overall_doc_score:.1%} - {doc_status}")

        results = {
            "status": doc_status,
            "overall_score": overall_doc_score,
            "total_modules": total_modules,
            "total_classes": total_classes,
            "total_methods": total_methods,
            "checks": documentation_checks
        }

        self.validation_results["documentation_check"] = results

    def generate_final_report(self):
        """Generate comprehensive final validation report."""
        print("\n" + "=" * 80)
        print("ğŸ“Š TASK 5.1 VALIDATION REPORT")
        print("=" * 80)

        # Count successes and failures
        file_structure_success = sum(1 for result in self.validation_results["file_structure"].values()
                                   if isinstance(result, dict) and result.get("status", "").startswith("âœ…"))
        file_structure_total = len(self.validation_results["file_structure"])

        interface_success = sum(1 for result in self.validation_results["module_interfaces"].values()
                              if isinstance(result, dict) and result.get("status") == "âœ… COMPLETE")
        interface_total = len(self.validation_results["module_interfaces"])

        config_success = sum(1 for result in self.validation_results["configuration_validation"].values()
                           if isinstance(result, dict) and result.get("status") == "âœ… COMPLETE")
        config_total = len(self.validation_results["configuration_validation"])

        doc_status = self.validation_results["documentation_check"].get("status", "âŒ POOR")

        print(f"\nğŸ“ File Structure: {file_structure_success}/{file_structure_total} files present")
        print(f"ğŸ” Module Interfaces: {interface_success}/{interface_total} modules complete")
        print(f"âš™ï¸ Configurations: {config_success}/{config_total} configurations complete")
        print(f"ğŸ“š Documentation: {doc_status}")

        # Calculate overall score
        scores = [
            file_structure_success / max(file_structure_total, 1),
            interface_success / max(interface_total, 1),
            config_success / max(config_total, 1),
            self.validation_results["documentation_check"].get("overall_score", 0)
        ]

        overall_score = sum(scores) / len(scores)

        if overall_score >= 0.9:
            overall_status = "ğŸ‰ EXCELLENT - PRODUCTION READY"
        elif overall_score >= 0.7:
            overall_status = "âœ… GOOD - READY FOR INTEGRATION"
        elif overall_score >= 0.5:
            overall_status = "âš ï¸ ADEQUATE - NEEDS MINOR IMPROVEMENTS"
        else:
            overall_status = "âŒ POOR - SIGNIFICANT WORK NEEDED"

        print(f"\nğŸ¯ OVERALL SCORE: {overall_score:.1%}")
        print(f"ğŸš€ STATUS: {overall_status}")

        # Detailed component summary
        print(f"\nğŸ“‹ COMPONENT SUMMARY:")
        total_files = 0
        total_lines = 0

        for filename, info in self.validation_results["file_structure"].items():
            if isinstance(info, dict) and "size_bytes" in info:
                total_files += 1
                if filename in self.validation_results["module_interfaces"]:
                    line_count = self.validation_results["module_interfaces"][filename.replace(".py", "")].get("line_count", 0)
                    total_lines += line_count
                    print(f"  âœ… {filename}: {info['size_bytes']:,} bytes, {line_count:,} lines")

        print(f"\nğŸ“Š IMPLEMENTATION METRICS:")
        print(f"  â€¢ Total Files: {total_files}")
        print(f"  â€¢ Total Lines of Code: {total_lines:,}")
        print(f"  â€¢ Average File Size: {total_lines//max(total_files,1):,} lines")

        # Task completion status
        if overall_score >= 0.8:
            task_status = "âœ… TASK 5.1 COMPLETED SUCCESSFULLY"
            ready_status = "ğŸš€ READY FOR PRODUCTION"
        else:
            task_status = "âš ï¸ TASK 5.1 PARTIALLY COMPLETED"
            ready_status = "ğŸ”§ REQUIRES ADDITIONAL WORK"

        print(f"\n{task_status}")
        print(f"{ready_status}")

        self.validation_results["overall_status"] = overall_status
        self.validation_results["overall_score"] = overall_score
        self.validation_results["total_files"] = total_files
        self.validation_results["total_lines"] = total_lines


def main():
    """Run the Task 5.1 validation."""
    validator = Task5_1Validator()
    results = validator.validate_all()

    # Save results to file
    import json
    results_file = Path(__file__).parent / "task_5_1_validation_results.json"

    # Convert results to JSON-serializable format
    json_results = json.dumps(results, indent=2, default=str)

    with open(results_file, 'w') as f:
        f.write(json_results)

    print(f"\nğŸ’¾ Validation results saved to: {results_file}")

    return results


if __name__ == "__main__":
    main()
