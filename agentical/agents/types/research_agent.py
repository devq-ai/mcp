"""
Research Agent Implementation for Agentical Framework

This module provides the ResearchAgent implementation for research tasks,
information gathering, analysis, and knowledge synthesis operations.

Features:
- Information gathering and analysis
- Literature review and synthesis
- Data collection and validation
- Report generation and documentation
- Citation management
- Research methodology assistance
- Knowledge graph construction
- Trend analysis and insights
"""

from typing import Dict, Any, List, Optional, Set, Union, Tuple
from datetime import datetime
import asyncio
import json
from pathlib import Path

import logfire
from pydantic import BaseModel, Field

from agentical.agents.enhanced_base_agent import EnhancedBaseAgent
from agentical.db.models.agent import AgentType, AgentStatus
from agentical.core.exceptions import AgentExecutionError, ValidationError
from agentical.core.structured_logging import StructuredLogger, OperationType, AgentPhase


class ResearchRequest(BaseModel):
    """Request model for research operations."""
    topic: str = Field(..., description="Research topic or question")
    research_type: str = Field(..., description="Type of research (literature_review, data_analysis, market_research)")
    scope: str = Field(default="comprehensive", description="Research scope (focused, comprehensive, exploratory)")
    sources: Optional[List[str]] = Field(default=None, description="Preferred information sources")
    depth_level: int = Field(default=3, description="Research depth level (1-5)")
    time_range: Optional[str] = Field(default=None, description="Time range for research (e.g., '2020-2024')")
    language: str = Field(default="en", description="Language for research sources")


class LiteratureReviewRequest(BaseModel):
    """Request model for literature review operations."""
    research_question: str = Field(..., description="Main research question")
    keywords: List[str] = Field(..., description="Keywords for literature search")
    databases: List[str] = Field(..., description="Academic databases to search")
    publication_years: Tuple[int, int] = Field(..., description="Start and end years for publications")
    study_types: Optional[List[str]] = Field(default=None, description="Types of studies to include")
    quality_criteria: Optional[Dict[str, Any]] = Field(default=None, description="Quality assessment criteria")


class DataAnalysisRequest(BaseModel):
    """Request model for research data analysis."""
    data_source: str = Field(..., description="Data source for analysis")
    analysis_method: str = Field(..., description="Analysis method to use")
    research_hypothesis: Optional[str] = Field(default=None, description="Research hypothesis to test")
    variables: List[str] = Field(..., description="Variables to analyze")
    statistical_tests: Optional[List[str]] = Field(default=None, description="Statistical tests to perform")
    visualization_type: Optional[str] = Field(default=None, description="Type of visualization needed")


class ResearchAgent(EnhancedBaseAgent[ResearchRequest, Dict[str, Any]]):
    """
    Specialized agent for research tasks and knowledge synthesis.

    Capabilities:
    - Information gathering and analysis
    - Literature review and synthesis
    - Data collection and validation
    - Report generation
    - Citation management
    - Research methodology
    - Knowledge synthesis
    - Trend analysis
    """

    def __init__(
        self,
        agent_id: str,
        name: str = "ResearchAgent",
        description: str = "Specialized agent for research and knowledge synthesis",
        **kwargs
    ):
        super().__init__(
            agent_id=agent_id,
            name=name,
            description=description,
            agent_type=AgentType.RESEARCH_AGENT,
            **kwargs
        )

        # Research-specific configuration
        self.research_types = {
            "literature_review": "Systematic review of existing literature",
            "data_analysis": "Statistical and qualitative data analysis",
            "market_research": "Market trends and competitive analysis",
            "survey_research": "Survey design and analysis",
            "experimental": "Experimental design and analysis",
            "case_study": "Case study research and analysis",
            "ethnographic": "Ethnographic and qualitative research"
        }

        self.information_sources = {
            "academic": ["pubmed", "google_scholar", "ieee", "acm", "arxiv"],
            "news": ["reuters", "ap_news", "bbc", "financial_times"],
            "government": ["census", "bureau_of_labor", "cdc", "fda"],
            "industry": ["gartner", "forrester", "idc", "mckinsey"],
            "social": ["twitter", "reddit", "linkedin", "stack_overflow"]
        }

        self.analysis_methods = [
            "content_analysis", "thematic_analysis", "statistical_analysis",
            "meta_analysis", "systematic_review", "grounded_theory",
            "sentiment_analysis", "trend_analysis", "comparative_analysis"
        ]

        self.citation_formats = [
            "apa", "mla", "chicago", "harvard", "ieee", "vancouver"
        ]

    @classmethod
    def get_capabilities(cls) -> List[str]:
        """Get list of agent capabilities."""
        return [
            "information_gathering",
            "literature_review",
            "data_analysis",
            "report_generation",
            "citation_management",
            "research_methodology",
            "knowledge_synthesis",
            "trend_analysis",
            "survey_design",
            "statistical_analysis",
            "content_analysis",
            "market_research",
            "competitive_analysis",
            "academic_writing",
            "research_planning",
            "hypothesis_testing",
            "data_visualization",
            "research_validation",
            "source_verification",
            "evidence_synthesis"
        ]

    async def _execute_core_logic(
        self,
        request: ResearchRequest,
        correlation_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Execute core research logic.

        Args:
            request: Research request
            correlation_context: Optional correlation context

        Returns:
            Research results with findings and analysis
        """
        with logfire.span(
            "ResearchAgent.execute_core_logic",
            agent_id=self.agent_id,
            topic=request.topic,
            research_type=request.research_type
        ):
            self.logger.log_operation(
                OperationType.AGENT_EXECUTION,
                AgentPhase.EXECUTION,
                {
                    "topic": request.topic,
                    "research_type": request.research_type,
                    "scope": request.scope,
                    "depth_level": request.depth_level
                },
                correlation_context
            )

            try:
                # Validate research type
                if request.research_type not in self.research_types:
                    raise ValidationError(f"Unsupported research type: {request.research_type}")

                # Execute research based on type
                if request.research_type == "literature_review":
                    result = await self._conduct_literature_review(request)
                elif request.research_type == "data_analysis":
                    result = await self._perform_data_analysis(request)
                elif request.research_type == "market_research":
                    result = await self._conduct_market_research(request)
                elif request.research_type == "survey_research":
                    result = await self._design_survey_research(request)
                else:
                    result = await self._conduct_general_research(request)

                # Add metadata
                result.update({
                    "topic": request.topic,
                    "research_type": request.research_type,
                    "scope": request.scope,
                    "depth_level": request.depth_level,
                    "timestamp": datetime.utcnow().isoformat(),
                    "agent_id": self.agent_id
                })

                logfire.info(
                    "Research completed",
                    agent_id=self.agent_id,
                    topic=request.topic,
                    research_type=request.research_type,
                    sources_found=result.get("summary", {}).get("sources_analyzed", 0)
                )

                return result

            except Exception as e:
                logfire.error(
                    "Research failed",
                    agent_id=self.agent_id,
                    error=str(e),
                    topic=request.topic
                )
                raise AgentExecutionError(f"Research failed: {str(e)}")

    async def _conduct_literature_review(self, request: ResearchRequest) -> Dict[str, Any]:
        """Conduct systematic literature review."""

        # Mock literature review results
        review_results = {
            "summary": {
                "total_sources": 156,
                "sources_analyzed": 89,
                "relevant_sources": 67,
                "time_span": "2019-2024",
                "search_strategy": "Systematic database search with inclusion/exclusion criteria"
            },
            "key_findings": [
                {
                    "theme": "Emerging Trends",
                    "description": "AI integration increasing across multiple domains",
                    "supporting_studies": 23,
                    "confidence_level": "high"
                },
                {
                    "theme": "Methodological Approaches",
                    "description": "Mixed-methods research becoming more prevalent",
                    "supporting_studies": 18,
                    "confidence_level": "medium"
                },
                {
                    "theme": "Future Directions",
                    "description": "Need for interdisciplinary collaboration",
                    "supporting_studies": 15,
                    "confidence_level": "medium"
                }
            ],
            "gaps_identified": [
                "Limited longitudinal studies on long-term effects",
                "Underrepresentation of diverse populations",
                "Need for standardized measurement tools"
            ],
            "methodology_assessment": {
                "study_quality": "Generally high",
                "bias_assessment": "Low to moderate risk",
                "heterogeneity": "Moderate"
            },
            "citations": [
                {
                    "authors": "Smith, J., Johnson, A.",
                    "title": "Advances in Research Methodology",
                    "journal": "Journal of Research Methods",
                    "year": 2023,
                    "volume": 45,
                    "pages": "123-145"
                }
            ]
        }

        return {
            "success": True,
            "literature_review": review_results,
            "operation": "literature_review"
        }

    async def _perform_data_analysis(self, request: ResearchRequest) -> Dict[str, Any]:
        """Perform research data analysis."""

        # Mock data analysis results
        analysis_results = {
            "summary": {
                "dataset_size": 1250,
                "variables_analyzed": 15,
                "statistical_tests": 8,
                "significance_level": 0.05,
                "analysis_duration": 120.5
            },
            "descriptive_statistics": {
                "mean_age": 34.2,
                "std_age": 12.8,
                "gender_distribution": {"male": 48.2, "female": 51.8},
                "missing_data_percentage": 3.2
            },
            "inferential_statistics": [
                {
                    "test": "t-test",
                    "variables": ["group1", "group2"],
                    "statistic": 2.45,
                    "p_value": 0.014,
                    "significant": True
                },
                {
                    "test": "chi-square",
                    "variables": ["category_a", "category_b"],
                    "statistic": 8.96,
                    "p_value": 0.003,
                    "significant": True
                }
            ],
            "effect_sizes": [
                {"comparison": "group1 vs group2", "cohens_d": 0.42, "interpretation": "medium"}
            ],
            "visualizations": [
                {"type": "histogram", "variable": "age", "file": "age_distribution.png"},
                {"type": "scatter", "variables": ["x", "y"], "file": "correlation_plot.png"}
            ],
            "conclusions": [
                "Significant difference found between treatment groups",
                "Effect size indicates practical significance",
                "Results support the research hypothesis"
            ]
        }

        return {
            "success": True,
            "data_analysis": analysis_results,
            "operation": "data_analysis"
        }

    async def _conduct_market_research(self, request: ResearchRequest) -> Dict[str, Any]:
        """Conduct market research and analysis."""

        # Mock market research results
        market_results = {
            "summary": {
                "market_size": "$2.5B",
                "growth_rate": "12.5% CAGR",
                "time_horizon": "2024-2029",
                "geographic_scope": "Global",
                "segments_analyzed": 6
            },
            "market_segments": [
                {
                    "segment": "Enterprise Solutions",
                    "market_share": 45.2,
                    "growth_rate": 15.3,
                    "key_players": ["Company A", "Company B", "Company C"]
                },
                {
                    "segment": "SMB Solutions",
                    "market_share": 28.7,
                    "growth_rate": 18.9,
                    "key_players": ["Company D", "Company E"]
                }
            ],
            "competitive_landscape": {
                "market_concentration": "Moderately concentrated",
                "top_3_market_share": 62.5,
                "competitive_intensity": "High",
                "barriers_to_entry": "Medium"
            },
            "trends": [
                {
                    "trend": "AI Integration",
                    "impact": "High",
                    "timeline": "2024-2025",
                    "description": "Increased adoption of AI-powered solutions"
                },
                {
                    "trend": "Sustainability Focus",
                    "impact": "Medium",
                    "timeline": "2024-2026",
                    "description": "Growing emphasis on sustainable practices"
                }
            ],
            "opportunities": [
                "Underserved international markets",
                "Integration with emerging technologies",
                "Vertical-specific solutions"
            ],
            "threats": [
                "New market entrants",
                "Regulatory changes",
                "Economic uncertainty"
            ]
        }

        return {
            "success": True,
            "market_research": market_results,
            "operation": "market_research"
        }

    async def _design_survey_research(self, request: ResearchRequest) -> Dict[str, Any]:
        """Design survey research methodology."""

        # Mock survey design results
        survey_design = {
            "research_objective": request.topic,
            "target_population": "Adults aged 18-65 in urban areas",
            "sample_size": {
                "recommended": 384,
                "confidence_level": 95,
                "margin_of_error": 5,
                "response_rate_estimate": 25
            },
            "sampling_method": "Stratified random sampling",
            "survey_structure": {
                "total_questions": 25,
                "demographic_questions": 6,
                "likert_scale_questions": 12,
                "open_ended_questions": 4,
                "multiple_choice_questions": 3
            },
            "question_examples": [
                {
                    "question": "How satisfied are you with current solutions?",
                    "type": "likert_5",
                    "scale": "Very Dissatisfied to Very Satisfied"
                },
                {
                    "question": "What features are most important to you?",
                    "type": "ranking",
                    "options": ["Feature A", "Feature B", "Feature C"]
                }
            ],
            "data_collection": {
                "method": "Online survey",
                "platform": "Qualtrics",
                "estimated_duration": "8-12 minutes",
                "pilot_test_size": 50
            },
            "analysis_plan": [
                "Descriptive statistics for all variables",
                "Correlation analysis between key variables",
                "Regression analysis for predictive modeling",
                "Thematic analysis for open-ended responses"
            ]
        }

        return {
            "success": True,
            "survey_design": survey_design,
            "operation": "survey_research"
        }

    async def _conduct_general_research(self, request: ResearchRequest) -> Dict[str, Any]:
        """Conduct general research on the specified topic."""

        # Mock general research results
        research_results = {
            "summary": {
                "sources_consulted": 45,
                "credible_sources": 38,
                "publication_timeframe": "2020-2024",
                "research_depth": request.depth_level,
                "scope": request.scope
            },
            "key_findings": [
                f"Primary insight about {request.topic}",
                f"Secondary observation regarding {request.topic}",
                f"Emerging trends in {request.topic}"
            ],
            "evidence_strength": {
                "strong_evidence": 15,
                "moderate_evidence": 18,
                "limited_evidence": 5
            },
            "source_breakdown": {
                "academic_papers": 20,
                "industry_reports": 12,
                "news_articles": 8,
                "government_sources": 5
            },
            "recommendations": [
                "Further research needed in specific area A",
                "Strong evidence supports conclusion B",
                "Mixed evidence requires careful interpretation for topic C"
            ]
        }

        return {
            "success": True,
            "research_results": research_results,
            "operation": "general_research"
        }

    async def conduct_literature_review(self, request: LiteratureReviewRequest) -> Dict[str, Any]:
        """
        Conduct systematic literature review.

        Args:
            request: Literature review request

        Returns:
            Comprehensive literature review results
        """
        with logfire.span(
            "ResearchAgent.conduct_literature_review",
            agent_id=self.agent_id,
            research_question=request.research_question
        ):
            try:
                # Mock comprehensive literature review
                review_results = {
                    "search_strategy": {
                        "databases_searched": request.databases,
                        "keywords_used": request.keywords,
                        "publication_years": f"{request.publication_years[0]}-{request.publication_years[1]}",
                        "initial_results": 2456,
                        "after_screening": 156,
                        "final_included": 89
                    },
                    "study_characteristics": {
                        "study_types": {
                            "randomized_controlled_trials": 25,
                            "cohort_studies": 18,
                            "case_control_studies": 15,
                            "cross_sectional_studies": 31
                        },
                        "geographic_distribution": {
                            "north_america": 35,
                            "europe": 28,
                            "asia": 20,
                            "other": 6
                        }
                    },
                    "synthesis_results": {
                        "main_findings": [
                            "Consistent evidence for primary outcome",
                            "Moderate evidence for secondary outcomes",
                            "Limited evidence for long-term effects"
                        ],
                        "heterogeneity": "Moderate (IÂ² = 45%)",
                        "publication_bias": "Low risk",
                        "quality_assessment": "Generally high quality"
                    }
                }

                logfire.info(
                    "Literature review completed",
                    agent_id=self.agent_id,
                    studies_included=review_results["search_strategy"]["final_included"]
                )

                return {"success": True, "literature_review": review_results}

            except Exception as e:
                logfire.error(
                    "Literature review failed",
                    agent_id=self.agent_id,
                    error=str(e)
                )
                raise AgentExecutionError(f"Literature review failed: {str(e)}")

    def get_default_configuration(self) -> Dict[str, Any]:
        """Get default configuration for research agent."""
        return {
            "max_sources_per_query": 100,
            "default_depth_level": 3,
            "default_scope": "comprehensive",
            "citation_format": "apa",
            "quality_threshold": 0.7,
            "credibility_check": True,
            "fact_verification": True,
            "bias_detection": True,
            "source_diversity": True,
            "temporal_analysis": True,
            "trend_identification": True,
            "statistical_analysis": True,
            "visualization_generation": True,
            "report_generation": True,
            "research_types": self.research_types,
            "information_sources": self.information_sources,
            "analysis_methods": self.analysis_methods,
            "citation_formats": self.citation_formats
        }

    def validate_configuration(self, config: Dict[str, Any]) -> bool:
        """Validate agent configuration."""
        required_fields = ["max_sources_per_query", "default_depth_level", "quality_threshold"]

        for field in required_fields:
            if field not in config:
                raise ValidationError(f"Missing required configuration field: {field}")

        # Validate depth level
        depth = config.get("default_depth_level", 0)
        if not 1 <= depth <= 5:
            raise ValidationError("default_depth_level must be between 1 and 5")

        # Validate quality threshold
        quality = config.get("quality_threshold", 0)
        if not 0 <= quality <= 1:
            raise ValidationError("quality_threshold must be between 0 and 1")

        # Validate max sources
        if config.get("max_sources_per_query", 0) <= 0:
            raise ValidationError("max_sources_per_query must be positive")

        return True
